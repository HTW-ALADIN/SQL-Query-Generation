{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypwhURKJ_Cj3"
      },
      "outputs": [],
      "source": [
        "!wget https://datasets.imdbws.com/name.basics.tsv.gz\n",
        "!wget https://datasets.imdbws.com/title.akas.tsv.gz\n",
        "!wget https://datasets.imdbws.com/title.basics.tsv.gz\n",
        "!wget https://datasets.imdbws.com/title.crew.tsv.gz\n",
        "!wget https://datasets.imdbws.com/title.episode.tsv.gz\n",
        "!wget https://datasets.imdbws.com/title.principals.tsv.gz\n",
        "!wget https://datasets.imdbws.com/title.ratings.tsv.gz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gzip"
      ],
      "metadata": {
        "id": "Yh_pZNueACny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helper functions"
      ],
      "metadata": {
        "id": "lYG02yBi8J7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Generator, List, Tuple, Dict, Set\n",
        "\n",
        "def iterate_compressed_file(file_handle: str, file_operation: str = \"rt\") -> Generator[str, None, None]:\n",
        "  with gzip.open(file_handle, file_operation) as f:\n",
        "    for line in f:\n",
        "      yield line\n",
        "\n",
        "def preprocess_line(line: str) -> List[str]:\n",
        "  return line.replace(\"\\\\N\", \"null\").replace(\"\\n\", \"\").replace(\"None\", \"null\").split(\"\\t\")\n",
        "\n",
        "def transform_id(id: str) -> int:\n",
        "  return int(id.replace(\"nm\", \"\").replace(\"tt\", \"\"))\n",
        "\n",
        "def extract_header(gen: Generator[str, None, None]) -> Tuple[List[str], Generator[str, None, None]]:\n",
        "  for line in gen:\n",
        "    header = preprocess_line(line)\n",
        "    break\n",
        "  return (header, (line for line in gen))\n",
        "\n",
        "def preprocess_joined_line(line: str) -> str:\n",
        "  return line.replace(\"\\n\", \"\\t\").replace(\"\\\\N\", \"null\").replace(\"None\", \"null\").strip(\"\\t\").split(\"\\t\")\n",
        "\n",
        "def zipped_generator(generators: List[Generator[str,None,None]]) -> Generator[str,None,None]:\n",
        "  \"\"\"\n",
        "  Iterates a list of generators at once until every generator is empty.\n",
        "  Empty generators return empty strings in the amount that was witnessed in the first iteration.\n",
        "  \"\"\"\n",
        "  stop_value = None\n",
        "  expected_output_lengths = []\n",
        "\n",
        "  generators_not_empty = True\n",
        "  i = 0\n",
        "  while generators_not_empty:\n",
        "    fields = []\n",
        "    for generator_index, generator in enumerate(generators):\n",
        "      generators_not_empty = False\n",
        "      partial_line = next(generator, stop_value)\n",
        "      if partial_line is not None:\n",
        "        generators_not_empty = True\n",
        "        processed_line = preprocess_line(partial_line)\n",
        "        if i == 0:\n",
        "          expected_output_lengths.append(len(processed_line))\n",
        "        fields.extend(processed_line)\n",
        "      else:\n",
        "        fields.extend([\"\"] * expected_output_lengths[generator_index])\n",
        "    i = i + 1\n",
        "    yield(fields)\n",
        "\n",
        "  # for lines in zip(*generators):\n",
        "  #   line = \"\".join(lines)\n",
        "  #   yield(preprocess_joined_line(line))\n",
        "\n",
        "def synched_iterator(generators: Tuple[Generator[str, None, None], Generator[str, None, None]], field_indices_to_sync: Tuple[int, int]) -> Generator[str, None, None]:\n",
        "  a, b = generators\n",
        "  a_i, b_i = field_indices_to_sync\n",
        "\n",
        "  y = None\n",
        "  for x in a:\n",
        "    x_fields = preprocess_line(x)\n",
        "    if not y:\n",
        "      y = next(b)\n",
        "      y_fields = preprocess_line(y)\n",
        "    if y_fields[b_i] != x_fields[a_i]:\n",
        "      yield [*x_fields, *[None for field in y_fields]]\n",
        "    else:\n",
        "      y = None\n",
        "      yield [*x_fields, *y_fields]\n",
        "\n",
        "from typing import Iterable\n",
        "\n",
        "\n",
        "def flatten(items):\n",
        "    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n",
        "    for x in items:\n",
        "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
        "            for sub_x in flatten(x):\n",
        "                yield sub_x\n",
        "        else:\n",
        "            yield x\n",
        "\n",
        "def set_to_dict(s: Set) -> Dict[str, int]:\n",
        "  return {\n",
        "      v: i\n",
        "      for i, v in enumerate(s)\n",
        "  }\n",
        "\n"
      ],
      "metadata": {
        "id": "I9-o4J8nCusR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## table preprocessing"
      ],
      "metadata": {
        "id": "FNH0tw__8Q6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def persist_to_tsv(file_name: str, data: List[List[any]]) -> None:\n",
        "  with open(f\"tables/{file_name}.tsv\", \"w\", encoding=\"utf8\") as f:\n",
        "    for fields in zip(*data):\n",
        "      line = \"\\t\".join([str(field) for field in fields])\n",
        "      f.write(f\"{line}\\n\")\n",
        "\n",
        "def extract_n_columns():\n",
        "  title_header, title_iterator = extract_header(iterate_compressed_file(files[\"title\"]))\n",
        "  person_header, person_iterator = extract_header(iterate_compressed_file(files[\"person\"]))\n",
        "  localization_header, localization_iterator = extract_header(iterate_compressed_file(files[\"localization\"]))\n",
        "  cast_header, cast_iterator = extract_header(iterate_compressed_file(files[\"cast\"]))\n",
        "\n",
        "\n",
        "  columns = {\n",
        "      \"years\": set(),\n",
        "      \"formats\": set(),\n",
        "      \"genres\": set(),\n",
        "      \"title_names\": set(),\n",
        "      \"professions\": set(),\n",
        "      \"names\": set(),\n",
        "      \"regions\": set(),\n",
        "      \"languages\": set(),\n",
        "      \"types\": set(),\n",
        "      \"attributes\": set(),\n",
        "      \"characters\": set(),\n",
        "  }\n",
        "\n",
        "  for line in title_iterator:\n",
        "    id, format, title_name, _, _, start_year, _, _, genre = preprocess_line(line)\n",
        "    columns[\"formats\"].add(format)\n",
        "    columns[\"genres\"].update(genre.split(\",\"))\n",
        "    columns[\"years\"].add(start_year)\n",
        "    columns[\"title_names\"].add(title_name)\n",
        "\n",
        "  for line in person_iterator:\n",
        "    id, name, birth_year, death_year, profession, _ = preprocess_line(line)\n",
        "    columns[\"professions\"].update(profession.split(\",\"))\n",
        "    columns[\"years\"].update([birth_year, death_year])\n",
        "    columns[\"names\"].add(name)\n",
        "\n",
        "  for line in localization_iterator:\n",
        "    _, _, localized_title, region, language, localization_type, localization_attribute, _ = preprocess_line(line)\n",
        "    columns[\"title_names\"].add(localized_title)\n",
        "    columns[\"regions\"].add(region)\n",
        "    columns[\"languages\"].add(language)\n",
        "    columns[\"attributes\"].add(localization_attribute)\n",
        "    columns[\"types\"].add(localization_type)\n",
        "\n",
        "  for line in cast_iterator:\n",
        "    _, _, _, job_category, job, character = preprocess_line(line)\n",
        "    columns[\"professions\"].update([job_category])\n",
        "    columns[\"characters\"].update(flatten(character.split(\",\")))\n",
        "\n",
        "  column_dicts = {}\n",
        "  for column, values in columns.items():\n",
        "    filtered_values = [value.replace(\"\\x02\", \" \") for value in values if value != \"\" and value != \"null\"]\n",
        "    column_dict = set_to_dict(filtered_values)\n",
        "    column_dicts[column] = column_dict\n",
        "    persist_to_tsv(column, [column_dict.values(), column_dict.keys()])\n",
        "\n",
        "  return column_dicts\n",
        "\n",
        "def id_of_value(column: str, value: any) -> int:\n",
        "  if value in column_dicts[column]:\n",
        "    return column_dicts[column][value]\n",
        "  return \"null\"\n",
        "\n",
        "def extract_episode_ids() -> Set[str]:\n",
        "  episode_header, episode_iterator = extract_header(iterate_compressed_file(files[\"episode\"]))\n",
        "\n",
        "  titles_to_ignore = set()\n",
        "  for line in episode_iterator:\n",
        "    fields = preprocess_line(line)\n",
        "    series_ids = transform_id(fields[0]), transform_id(fields[1])\n",
        "    titles_to_ignore.update(series_ids)\n",
        "\n",
        "  return titles_to_ignore\n",
        "\n",
        "\n",
        "def process_titles() -> set:\n",
        "  title_header, title_iterator = extract_header(iterate_compressed_file(files[\"title\"]))\n",
        "  rating_header, rating_iterator = extract_header(iterate_compressed_file(files[\"rating\"]))\n",
        "\n",
        "  title_ids_to_keep = set()\n",
        "\n",
        "  with open(\"tables/title.tsv\", \"w\", encoding=\"utf8\") as title_f:\n",
        "    with open(\"tables/title_genre.tsv\", \"w\", encoding=\"utf8\") as title_genre_f:\n",
        "      for line in synched_iterator([title_iterator, rating_iterator], [0, 0]):\n",
        "        title_id, format, primary_title, _, is_adult, start_year, _, runtime, genre, _, average_rating, num_votes = line\n",
        "        numeric_title_id = transform_id(title_id)\n",
        "\n",
        "        if numeric_title_id in titles_to_ignore:\n",
        "          continue\n",
        "        title_ids_to_keep.add(numeric_title_id)\n",
        "\n",
        "        format_id = id_of_value(\"formats\", format)\n",
        "        title_name_id = id_of_value(\"title_names\", primary_title)\n",
        "        release_year_id = id_of_value(\"years\", start_year)\n",
        "        title_line = f\"{numeric_title_id}\\t{format_id}\\t{title_name_id}\\t{release_year_id}\\t{is_adult}\\t{runtime}\\t{average_rating}\\t{num_votes}\\n\"\n",
        "        title_f.write(title_line)\n",
        "\n",
        "        for g in genre.split(\",\"):\n",
        "          if g != \"null\":\n",
        "            genre_id = id_of_value(\"genres\", g)\n",
        "            title_genre_f.write(f\"{numeric_title_id}\\t{genre_id}\\n\")\n",
        "\n",
        "  return title_ids_to_keep\n",
        "\n",
        "\n",
        "def process_persons() -> None:\n",
        "  person_header, person_iterator = extract_header(iterate_compressed_file(files[\"person\"]))\n",
        "  crew_header, crew_iterator = extract_header(iterate_compressed_file(files[\"crew\"]))\n",
        "  cast_header, cast_iterator = extract_header(iterate_compressed_file(files[\"cast\"]))\n",
        "\n",
        "  persons_to_keep = set()\n",
        "\n",
        "  with open(\"tables/person_title.tsv\", \"w\", encoding=\"utf8\") as person_title_f:\n",
        "    for line in crew_iterator:\n",
        "      title_id, director_id, writer_id = preprocess_line(line)\n",
        "      numeric_title_id = transform_id(title_id)\n",
        "\n",
        "      if numeric_title_id in title_ids_to_keep:\n",
        "\n",
        "        if director_id != \"null\":\n",
        "          for d_id in director_id.split(\",\"):\n",
        "            numeric_director_id = transform_id(d_id)\n",
        "            persons_to_keep.add(numeric_director_id)\n",
        "\n",
        "            profession_id = column_dicts[\"professions\"][\"director\"]\n",
        "            director_line = f\"{numeric_director_id}\\t{numeric_title_id}\\t{profession_id}\\n\"\n",
        "            person_title_f.write(director_line)\n",
        "\n",
        "        if writer_id != \"null\":\n",
        "          for w_id in writer_id.split(\",\"):\n",
        "            numeric_writer_id = transform_id(w_id)\n",
        "            persons_to_keep.add(numeric_writer_id)\n",
        "\n",
        "            profession_id = column_dicts[\"professions\"][\"director\"]\n",
        "            writer_line = f\"{numeric_writer_id}\\t{numeric_title_id}\\t{profession_id}\\n\"\n",
        "            person_title_f.write(writer_line)\n",
        "\n",
        "    for line in cast_iterator:\n",
        "      title_id, _, person_id, job, _, _ = preprocess_line(line)\n",
        "      numeric_title_id = transform_id(title_id)\n",
        "\n",
        "      if numeric_title_id in title_ids_to_keep:\n",
        "        numeric_person_id = transform_id(person_id)\n",
        "        persons_to_keep.add(numeric_person_id)\n",
        "\n",
        "        profession_id = id_of_value(\"professions\", job)\n",
        "        if profession_id != \"null\":\n",
        "          cast_line = f\"{numeric_person_id}\\t{numeric_title_id}\\t{profession_id}\\n\"\n",
        "          person_title_f.write(cast_line)\n",
        "\n",
        "  with open(\"tables/person.tsv\", \"w\", encoding=\"utf8\") as person_f:\n",
        "    with open(\"tables/person_profession.tsv\", \"w\", encoding=\"utf8\") as person_profession_f:\n",
        "      for line in person_iterator:\n",
        "        person_id, name, birth_year, death_year, profession, _ = preprocess_line(line)\n",
        "        numeric_person_id = transform_id(person_id)\n",
        "\n",
        "        if numeric_person_id in persons_to_keep:\n",
        "          name_id = id_of_value(\"names\", name)\n",
        "          birth_year_id = id_of_value(\"years\", birth_year)\n",
        "          death_year_id = id_of_value(\"years\", death_year)\n",
        "          person_line = f\"{numeric_person_id}\\t{name_id}\\t{birth_year_id}\\t{death_year_id}\\n\"\n",
        "          person_f.write(person_line)\n",
        "\n",
        "          for p in profession.split(\",\"):\n",
        "            profession_id = id_of_value(\"professions\", p)\n",
        "            if profession_id != \"null\":\n",
        "              person_profession_line = f\"{numeric_person_id}\\t{profession_id}\\n\"\n",
        "              person_profession_f.write(person_profession_line)\n",
        "\n",
        "\n",
        "def process_localization() -> None:\n",
        "  localization_header, localization_iterator = extract_header(iterate_compressed_file(files[\"localization\"]))\n",
        "\n",
        "  with open(\"tables/localization.tsv\", \"w\", encoding=\"utf8\") as localization_f:\n",
        "    localization_id = 0\n",
        "    for line in localization_iterator:\n",
        "      title_id, _, title_name, region, language, _, _, is_original_title = preprocess_line(line)\n",
        "\n",
        "      if is_original_title == \"0\":\n",
        "\n",
        "        numeric_title_id = transform_id(title_id)\n",
        "        if numeric_title_id in title_ids_to_keep:\n",
        "\n",
        "          title_name_id = id_of_value(\"title_names\", region)\n",
        "          region_id = id_of_value(\"regions\", region)\n",
        "          language_id = id_of_value(\"languages\", language)\n",
        "          localization_line = f\"{localization_id}\\t{numeric_title_id}\\t{title_name_id}\\t{region_id}\\t{language_id}\\n\"\n",
        "          localization_f.write(localization_line)\n",
        "\n",
        "          localization_id = localization_id + 1"
      ],
      "metadata": {
        "id": "AM9Q5__ns8GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## imdb_schema table overview"
      ],
      "metadata": {
        "id": "0ipvdlnK8Zcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = {\n",
        "    \"person\": \"name.basics.tsv.gz\",\n",
        "    \"localization\": \"title.akas.tsv.gz\",\n",
        "    \"title\": \"title.basics.tsv.gz\",\n",
        "    \"crew\": \"title.crew.tsv.gz\",\n",
        "    \"cast\": \"title.principals.tsv.gz\",\n",
        "    \"rating\": \"title.ratings.tsv.gz\",\n",
        "    \"episode\": \"title.episode.tsv.gz\"\n",
        "}"
      ],
      "metadata": {
        "id": "in4HhnbGBGtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "path = \"tables\"\n",
        "if not os.path.exists(path):\n",
        "  os.makedirs(path)"
      ],
      "metadata": {
        "id": "7n-Dl7I7E3DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "process columns with 1-n or n-m cardinalities\n",
        "\n",
        "(\n",
        "      years,\n",
        "      formats,\n",
        "      genres,\n",
        "      title_names,\n",
        "      professions,\n",
        "      names,\n",
        "      regions,\n",
        "      languages\n",
        ")"
      ],
      "metadata": {
        "id": "_AAevX0M8gPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_dicts = extract_n_columns()"
      ],
      "metadata": {
        "id": "i8gbJdfQ2neJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generate lookup-set for filtering titles that are not movies or games (e.g. tv-series)"
      ],
      "metadata": {
        "id": "KR2rs70A9e8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles_to_ignore = extract_episode_ids()\n",
        "len(titles_to_ignore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6JMt48RPNOZ",
        "outputId": "ca0b265d-887b-4c93-ad60-e2d61619d803"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7945657"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter and process title and title_genre tables"
      ],
      "metadata": {
        "id": "5cJ4hzK8_Bcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_ids_to_keep = process_titles()"
      ],
      "metadata": {
        "id": "OwuJtF-btROe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter and process person, person_title and person_profession tables"
      ],
      "metadata": {
        "id": "XNW8VYhA_JYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_persons()"
      ],
      "metadata": {
        "id": "kG4bYWYLn6DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "filter and process localization table"
      ],
      "metadata": {
        "id": "I4Gz0JZl_U6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "process_localization()"
      ],
      "metadata": {
        "id": "6rwjEQRgawIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fixing wrongly assigned null values (None -> null) title\n",
        "with open(\"tables/title.tsv\", \"r\", encoding=\"utf8\") as i_f:\n",
        "  with open(\"tables/titles.tsv\", \"w\", encoding=\"utf8\") as o_f:\n",
        "    for line in i_f:\n",
        "      new_line = line.replace(\"None\", \"null\")\n",
        "      o_f.write(new_line)"
      ],
      "metadata": {
        "id": "oDoOBpmw58GF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ./tables.zip ./tables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0STl7dbFEgM",
        "outputId": "8a27657e-8a2d-4543-871c-4ea71b303636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: tables/ (stored 0%)\n",
            "  adding: tables/regions.tsv (deflated 43%)\n",
            "  adding: tables/attributes.tsv (deflated 66%)\n",
            "  adding: tables/person_profession.tsv (deflated 75%)\n",
            "  adding: tables/formats.tsv (deflated 21%)\n",
            "  adding: tables/title_names.tsv (deflated 47%)\n",
            "  adding: tables/characters.tsv (deflated 56%)\n",
            "  adding: tables/types.tsv (deflated 57%)\n",
            "  adding: tables/title_genre.tsv (deflated 75%)\n",
            "  adding: tables/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: tables/professions.tsv (deflated 53%)\n",
            "  adding: tables/localization.tsv (deflated 80%)\n",
            "  adding: tables/person_title.tsv (deflated 70%)\n",
            "  adding: tables/titles.tsv (deflated 72%)\n",
            "  adding: tables/person.tsv (deflated 72%)\n",
            "  adding: tables/names.tsv (deflated 52%)\n",
            "  adding: tables/title.tsv (deflated 72%)\n",
            "  adding: tables/years.tsv (deflated 55%)\n",
            "  adding: tables/genres.tsv (deflated 26%)\n",
            "  adding: tables/languages.tsv (deflated 38%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"tables/person.tsv\", \"r\", encoding=\"utf8\") as f:\n",
        "  for line in f:\n",
        "    i, n = line.split(\"\\t\")\n",
        "    if len(s) != 4:\n",
        "      print(line)"
      ],
      "metadata": {
        "id": "z0gnkDnIOeD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # filtering person_ids that are not present in the person table\n",
        "# person_ids = set()\n",
        "# with open(\"person.tsv\", \"r\", encoding=\"utf8\") as person_f:\n",
        "#   for line in person_f:\n",
        "#     person_ids.add(line.split(\"\\t\")[0])\n",
        "\n",
        "# with open(\"person_title.tsv\", \"r\", encoding=\"utf8\") as person_title_f:\n",
        "#   with open(\"person_titles.tsv\", \"w\", encoding=\"utf8\") as new_person_title_f:\n",
        "#     for line in person_title_f:\n",
        "#       fields = line.split(\"\\t\")\n",
        "\n",
        "#       if len(fields) != 3:\n",
        "#         print(line)\n",
        "#       person_id = line.split(\"\\t\")[0]\n",
        "#       if person_id in person_ids and :\n",
        "#         new_person_title_f.write(line)\n",
        "\n",
        "i = 0\n",
        "with open(\"tables/person_title.tsv\", \"r\", encoding=\"utf8\") as person_title_f:\n",
        "  # with open(\"tables/person_titles.tsv\", \"w\", encoding=\"utf8\") as new_person_title_f:\n",
        "  for line in person_title_f:\n",
        "    p_id, t_id, pr_id = line.split(\"\\t\")\n",
        "\n",
        "    i = i+1\n",
        "    if i > 5500000:\n",
        "      print(line)\n",
        "    if i > 5500020:\n",
        "      break\n",
        "\n"
      ],
      "metadata": {
        "id": "v0T3S2SmEqgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}