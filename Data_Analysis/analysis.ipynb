{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('./data/mturk.csv')\n",
    "df.head(1)\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def calc_cohens_kappa(df, subject, rater_amount=3):\n",
    "\n",
    "    pivot_feature_df = df.pivot(index='HITId', columns='rater', values=subject)\n",
    "    pivot_feature_df.columns = ['Worker_A', 'Worker_B', 'Worker_C']\n",
    "    pivot_feature_df.reset_index(inplace=True)\n",
    "    pivot_feature_df.drop(columns=[\"HITId\"], inplace=True)\n",
    "    pivot_feature_df.fillna(0, inplace=True)\n",
    "    \n",
    "    pivot_feature_df_transposed = pivot_feature_df.transpose()\n",
    "    \n",
    "    data = np.zeros((rater_amount, rater_amount))\n",
    "    for j, k in list(itertools.combinations(range(rater_amount), r=2)):\n",
    "        # table = to_table(pivot_feature_df_transposed.iloc[:, [j, k]], bins=3)\n",
    "        # data[j, k] = cohens_kappa(table, return_results=False)\n",
    "        data[j, k] = cohen_kappa_score(pivot_feature_df_transposed.iloc[j], pivot_feature_df_transposed.iloc[k])\n",
    "\n",
    "        # rater_map = {\"0-1\": \"C\", \"0-2\": \"B\", \"1-2\": \"A\"}\n",
    "        # df_filtered = df.loc[df['rater'] != rater_map[f\"{j}-{k}\"]]\n",
    "        # data[j, k] = agreement_score(df_filtered, subject)\n",
    "    return [data, pivot_feature_df]\n",
    "\n",
    "import pingouin as pg\n",
    "\n",
    "def calc_icc(df, subject):\n",
    "    t_df = pd.melt(df, id_vars=['HITId', 'rater', \"typicality\", \"plausibility\", \"complexity\", \"human_like_quality\"])\n",
    "\n",
    "    icc = pg.intraclass_corr(data=t_df, targets='HITId', raters='rater', ratings='complexity', nan_policy='omit')\n",
    "    icc.set_index('Type')\n",
    "    return icc\n",
    "\n",
    "\n",
    "from agreement.utils.transform import pivot_table_frequency\n",
    "from agreement.utils.kernels import (\n",
    "    compute_weights, identity_kernel, linear_kernel, quadratic_kernel, ordinal_kernel,\n",
    "    radical_kernel, radio_kernel, circular_kernel, bipolar_kernel\n",
    ")\n",
    "from agreement.metrics import (\n",
    "    observed_agreement,\n",
    "    s_score,\n",
    "    cohens_kappa,\n",
    "    gwets_gamma,\n",
    "    krippendorffs_alpha,\n",
    "    scotts_pi\n",
    ")\n",
    "\n",
    "def agreement_score(data, subject, method=cohens_kappa, kernel=identity_kernel):\n",
    "    questions_answers_table = pivot_table_frequency(data[\"HITId\"], data[subject])\n",
    "    users_answers_table = pivot_table_frequency(data[\"rater\"], data[subject])\n",
    "\n",
    "    ob_ag = observed_agreement(questions_answers_table)\n",
    "    score = method(questions_answers_table, kernel)\n",
    "\n",
    "    if(method.__name__ == \"cohens_kappa\"):\n",
    "        score=cohens_kappa(questions_answers_table, users_answers_table, kernel)\n",
    "\n",
    "    return score, ob_ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "# import plotly\n",
    "import numpy as np\n",
    "\n",
    "# changing plotting backend to either plotly\n",
    "# pd.options.plotting.backend = \"plotly\"\n",
    "# ..matplotlib\n",
    "pd.options.plotting.backend = \"matplotlib\"\n",
    "# or/and seaborn (requires matplotlib backend)\n",
    "\n",
    "cmap=sns.color_palette(\"tab10\",as_cmap=True)\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze Query Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_query_composition(s):\n",
    "    selected_column_amount = len([m.start() for m in re.finditer('(?=,)', s.split(\"\\n\")[0])]) + 1\n",
    "    selected_aggregates = len([m.start() for m in re.finditer('(AVG|COUNT|MAX|MIN|SUM=,)', s.split(\"\\n\")[0])])\n",
    "    column_amount = len(re.findall('^.*JOIN.*$', s, re.MULTILINE)) + 1\n",
    "    group_by = len(re.findall('^.*GROUP.*$', s, re.MULTILINE))\n",
    "    order_by = len(re.findall('^.*ORDER.*$', s, re.MULTILINE))\n",
    "    where_line = re.findall('\\n(WHERE.*)\\n', s, re.MULTILINE)\n",
    "    where_predicate_amount = len(re.findall('^.*OR|AND.*$', where_line[0], re.MULTILINE)) + 1 if where_line else 0\n",
    "    having_line = re.findall('(HAVING.*)', s, re.MULTILINE)\n",
    "    having_predicate_amount = len(re.findall('^.*OR|AND.*$', having_line[0], re.MULTILINE)) + 1 if having_line else 0\n",
    "\n",
    "    return {\n",
    "        \"selected_column_amount\": selected_column_amount,\n",
    "        \"selected_aggregates\": selected_aggregates,\n",
    "        \"column_amount\": column_amount,\n",
    "        \"where_predicate_amount\": where_predicate_amount,\n",
    "        \"having_predicate_amount\": having_predicate_amount,\n",
    "        \"group_by\": group_by,\n",
    "        \"order_by\": order_by\n",
    "    }\n",
    "\n",
    "df[\"Input.query_composition\"] = df[\"Input.query\"].apply(extract_query_composition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Coordinates Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "query_composition_df = pd.DataFrame.from_records(df.loc[::3,\"Input.query_composition\"])\n",
    "query_composition_df[\"query\"] = df.loc[::3,\"Input.query\"]\n",
    "query_composition_df[\"average_complexity\"]  = list(df.groupby(\"HITId\")[\"complexity\"].mean())\n",
    "query_composition_df[\"rounded_average_complexity\"] = query_composition_df[\"average_complexity\"].apply(round)\n",
    "query_composition_df[\"HITId\"] = df.loc[::3,\"HITId\"]\n",
    "\n",
    "fig = px.parallel_categories(query_composition_df, width= 1000, dimensions= [\n",
    "    \"selected_column_amount\", \n",
    "                'selected_aggregates', \n",
    "                'column_amount',\n",
    "                'where_predicate_amount',\n",
    "                \"having_predicate_amount\",\n",
    "                \"group_by\",\n",
    "                \"order_by\"\n",
    "                ],\n",
    "    labels={\n",
    "        \"selected_column_amount\": \"Columns\",\n",
    "        \"selected_aggregates\": \"Aggregates\", \n",
    "        \"column_amount\": \"Tables\", \n",
    "        \"where_predicate_amount\": \"Where\", \n",
    "        \"having_predicate_amount\": \"Having\", \n",
    "        \"group_by\": \"Group\", \n",
    "        \"order_by\": \"Order\", \n",
    "        \"rounded_average_complexity\": \"\"\n",
    "   },\n",
    "   color=\"rounded_average_complexity\", \n",
    "   color_continuous_scale=px.colors.sequential.Inferno,                  \n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=17,  # Set the font size here\n",
    "        color=\"Black\"\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_yaxes(\n",
    "    autorange=False,\n",
    "    range = [-1,len(df)],\n",
    "    tick0=0,\n",
    "    dtick=1\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inter Rater Agreement (multi-rater) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_c = agreement_score(df, \"complexity\", krippendorffs_alpha, ordinal_kernel)\n",
    "as_p = agreement_score(df, \"plausibility\", krippendorffs_alpha, ordinal_kernel)\n",
    "as_t = agreement_score(df, \"typicality\", krippendorffs_alpha, ordinal_kernel)\n",
    "as_h = agreement_score(df, \"human_like_quality\", krippendorffs_alpha, radio_kernel)\n",
    "\n",
    "as_c, as_p, as_t, as_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity to SQL-Query\n",
    "###### !! Requires GPU for fast computation - utilize cached computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# b_similarities = []\n",
    "# n_similarities = []\n",
    "\n",
    "# for i in df.index:\n",
    "#   print(i)\n",
    "#   sql_sentences = df.iloc[i][\"Input.query\"]\n",
    "  \n",
    "#   english_sentences = [df.iloc[i][\"Input.baselineQuery\"], df.iloc[i][\"Input.nlQuery\"]]\n",
    "\n",
    "#   embeddings1 = model.encode(sql_sentences)\n",
    "#   embeddings2 = model.encode(english_sentences)\n",
    "#   similarities = model.similarity(embeddings1, embeddings2)\n",
    "#   [b,n] = similarities[0]\n",
    "#   b_similarities.append(b)\n",
    "#   n_similarities.append(n)\n",
    "\n",
    "# df[\"b_similarities\"] = b_similarities\n",
    "# df[\"n_similarities\"] = n_similarities\n",
    "\n",
    "\n",
    "# Load cached similarities\n",
    "import re\n",
    "\n",
    "df_with_similarities = pd.read_csv('./mturk_with_similarities.csv')\n",
    "df[\"b_similarities\"] = df_with_similarities[\"b_similarities\"].apply(lambda x: float(re.match(r\".*?\\((\\d*\\.\\d*)\\)\", x).groups()[0]))\n",
    "df[\"n_similarities\"] = df_with_similarities[\"n_similarities\"].apply(lambda x: float(re.match(r\".*?\\((\\d*\\.\\d*)\\)\", x).groups()[0]))\n",
    "\n",
    "df[\"b_similarities\"].mean(), df[\"n_similarities\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "\n",
    "def calc_readability(df, method):\n",
    "    return df[\"Input.baselineQuery\"].apply(method), df[\"Input.nlQuery\"].apply(method)\n",
    "[baseline, nl] = calc_readability(df, textstat.flesch_reading_ease)\n",
    "\n",
    "df[\"baselineQuery_Readability\"] = baseline\n",
    "df[\"nlQuery_Readability\"] = nl\n",
    "df[\"readability_dif\"] = nl - baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Quality Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textdescriptives as td\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.add_pipe('textdescriptives/all')\n",
    "\n",
    "def construct_spacy_docs(df, column):\n",
    "    return df[column].apply(nlp)\n",
    "\n",
    "df[\"query_doc\"] = construct_spacy_docs(df, \"Input.query\")\n",
    "df[\"baseline_doc\"] = construct_spacy_docs(df, \"Input.baselineQuery\")\n",
    "df[\"nl_doc\"] = construct_spacy_docs(df, \"Input.nlQuery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check(df, column):\n",
    "    return df[column].apply(lambda x: x._.passed_quality_check)\n",
    "\n",
    "df[\"baseline_text_quality\"] = quality_check(df, \"baseline_doc\")\n",
    "df[\"nl_text_quality\"] = quality_check(df, \"nl_doc\")\n",
    "\n",
    "df[\"baseline_text_quality\"].value_counts(), df[\"nl_text_quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_dependency_distance(df, column):\n",
    "    return df[column].apply(lambda x: x._.dependency_distance[\"dependency_distance_mean\"])\n",
    "\n",
    "df[\"baseline_dependency_distance\"] = calc_dependency_distance(df, \"baseline_doc\")\n",
    "df[\"nl_dependency_distance\"] = calc_dependency_distance(df, \"nl_doc\")\n",
    "\n",
    "df[\"baseline_dependency_distance\"].mean(), df[\"nl_dependency_distance\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_coherence(df, column):\n",
    "    return df[column].apply(lambda x: x._.coherence[\"first_order_coherence\"])\n",
    "\n",
    "df[\"baseline_coherence\"] = calc_coherence(df, \"baseline_doc\")\n",
    "df[\"nl_coherence\"] = calc_coherence(df, \"nl_doc\")\n",
    "\n",
    "df[\"baseline_coherence\"].mean(), df[\"nl_coherence\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import chrf_score, bleu_score, gleu_score\n",
    "\n",
    "def calc_translation_quality(df, method):\n",
    "    func1 = lambda x: method(x[\"Input.query\"], x[\"Input.baselineQuery\"])\n",
    "    func2 = lambda x: method(x[\"Input.query\"], x[\"Input.nlQuery\"])\n",
    "    return df.apply(func1, axis=1), df.apply(func2, axis=1)\n",
    "[baseline, nl] = calc_translation_quality(df, chrf_score.sentence_chrf)\n",
    "\n",
    "df[\"baselineQuery_quality\"] = baseline\n",
    "df[\"nlQuery_quality\"] = nl\n",
    "df[\"quality_dif\"] = baseline - nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Typicality and Plausibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_indecisive(x):\n",
    "    return len(x.value_counts()) > 2\n",
    "\n",
    "def indecisive_majority_vote(x):\n",
    "    average_vote = x.mean()\n",
    "    if average_vote >= 0:\n",
    "        return average_vote.round()\n",
    "    if average_vote < 0 and average_vote > -2:\n",
    "        return -1\n",
    "    else:\n",
    "        return -2\n",
    "    \n",
    "def decisive_majority_vote(x): \n",
    "    votes = x.value_counts()\n",
    "    return votes.idxmax()\n",
    "\n",
    "indecisive_plausibility_HITs = df.groupby('HITId')['plausibility'].apply(filter_indecisive)\n",
    "indecisive_plausibility_HIT_df = df[df[\"HITId\"].isin(list(indecisive_plausibility_HITs[indecisive_plausibility_HITs == True].index))]\n",
    "decisive_plausibility_HIT_df = df[df[\"HITId\"].isin(list(indecisive_plausibility_HITs[indecisive_plausibility_HITs == False].index))]\n",
    "\n",
    "indecisive_typicality_HITs = df.groupby('HITId')['typicality'].apply(filter_indecisive)\n",
    "indecisive_typicality_HIT_df = df[df[\"HITId\"].isin(list(indecisive_typicality_HITs[indecisive_typicality_HITs == True].index))]\n",
    "decisive_typicality_HIT_df = df[df[\"HITId\"].isin(list(indecisive_typicality_HITs[indecisive_typicality_HITs == False].index))]\n",
    "\n",
    "# indecisive_plausibility_HIT_df.groupby('HITId')['plausibility'].apply(indecisive_majority_vote).value_counts(),indecisive_typicality_HIT_df.groupby('HITId')['plausibility'].apply(indecisive_majority_vote).value_counts()\n",
    "\n",
    "indecisive_plausibility_HIT_df = indecisive_plausibility_HIT_df.groupby('HITId')['plausibility'].apply(indecisive_majority_vote).value_counts().to_frame().reset_index()\n",
    "new_rows = pd.DataFrame({\"plausibility\": [2, -2], \"count\": [0,0]})\n",
    "indecisive_plausibility_HIT_df = pd.concat([indecisive_plausibility_HIT_df, new_rows], ignore_index=True).sort_values(\"plausibility\")\n",
    "\n",
    "decisive_plausibility_HIT_df = decisive_plausibility_HIT_df.groupby('HITId')['plausibility'].apply(indecisive_majority_vote).value_counts().to_frame().reset_index()\n",
    "new_rows = pd.DataFrame({\"plausibility\": [-2], \"count\": [0]})\n",
    "decisive_plausibility_HIT_df = pd.concat([decisive_plausibility_HIT_df, new_rows], ignore_index=True).sort_values(\"plausibility\")\n",
    "\n",
    "\n",
    "indecisive_typicality_HIT_df = indecisive_typicality_HIT_df.groupby('HITId')['typicality'].apply(indecisive_majority_vote).value_counts().to_frame().reset_index()\n",
    "new_rows = pd.DataFrame({\"typicality\": [2, -2], \"count\": [0,0]})\n",
    "indecisive_typicality_HIT_df = pd.concat([indecisive_typicality_HIT_df, new_rows], ignore_index=True).sort_values(\"typicality\")\n",
    "\n",
    "\n",
    "decisive_typicality_HIT_df = decisive_typicality_HIT_df.groupby('HITId')['typicality'].apply(indecisive_majority_vote).value_counts().to_frame().reset_index()\n",
    "new_rows = pd.DataFrame({\"typicality\": [-2], \"count\": [0]})\n",
    "decisive_typicality_HIT_df = pd.concat([decisive_typicality_HIT_df, new_rows], ignore_index=True).sort_values(\"typicality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "indecisive_plausibility_HIT_df \n",
    "decisive_plausibility_HIT_df \n",
    "indecisive_typicality_HIT_df \n",
    "decisive_typicality_HIT_df\n",
    "\n",
    "pos_mut_pcts = np.array([20, 10, 5, 7.5, 30, 50])\n",
    "pos_cna_pcts = np.array([10, 0, 0, 7.5, 10, 0])\n",
    "pos_both_pcts = np.array([10, 0, 0, 0, 0, 0])\n",
    "neg_mut_pcts = np.array([10, 30, 5, 0, 10, 25])\n",
    "neg_cna_pcts = np.array([5, 0, 7.5, 0, 0, 10])\n",
    "neg_both_pcts = np.array([0, 0, 0, 0, 0, 10])\n",
    "genes = ['PIK3CA', 'PTEN', 'CDKN2A', 'FBXW7', 'KRAS', 'TP53']\n",
    "genes = decisive_typicality_HIT_df[\"typicality\"]\n",
    "\n",
    "# with sns.axes_style(\"white\"):\n",
    "#     sns.set_style(\"ticks\")\n",
    "#     sns.set_context(\"talk\")\n",
    "    \n",
    "# plot details\n",
    "bar_width = 0.35\n",
    "line_width = 1\n",
    "opacity = 0.7\n",
    "pos_bar_positions = np.arange(len(indecisive_plausibility_HIT_df[\"count\"]))\n",
    "neg_bar_positions = pos_bar_positions + bar_width\n",
    "\n",
    "# make bar plots\n",
    "hpv_pos_mut_bar = plt.bar(pos_bar_positions, decisive_plausibility_HIT_df[\"count\"], bar_width,\n",
    "                            color='#ff7f0e',\n",
    "                            edgecolor=\"grey\",\n",
    "                            linewidth=line_width,\n",
    "                            label='Plausibility')\n",
    "hpv_pos_cna_bar = plt.bar(pos_bar_positions, indecisive_plausibility_HIT_df[\"count\"], bar_width,\n",
    "                            bottom=decisive_plausibility_HIT_df[\"count\"],\n",
    "                            alpha=opacity,\n",
    "                            color='#ff9f0e',\n",
    "                            edgecolor=\"grey\",\n",
    "                            linewidth=line_width,\n",
    "                            hatch='//',\n",
    "                            label='Plausibility Indecisive')\n",
    "hpv_neg_mut_bar = plt.bar(neg_bar_positions, decisive_typicality_HIT_df[\"count\"], bar_width,\n",
    "                            color='#1f77b4',\n",
    "                            edgecolor=\"grey\",\n",
    "                            linewidth=line_width,\n",
    "                            label='Typicality')\n",
    "hpv_neg_cna_bar = plt.bar(neg_bar_positions, indecisive_typicality_HIT_df[\"count\"], bar_width,\n",
    "                            bottom=decisive_typicality_HIT_df[\"count\"],\n",
    "                            hatch='//',\n",
    "                            color='#97cbee',\n",
    "                            edgecolor=\"grey\",\n",
    "                            linewidth=line_width,\n",
    "                            label='Typicality Indecisive')\n",
    "plt.xticks(pos_bar_positions + bar_width/2, [\"Very\\nUnlikely\", \"Unlikely\", \"Neutral\", \"Likely\", \"Very\\nLikely\"])\n",
    "plt.ylabel('Amount')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "for i in range(cmap.N):\n",
    "    rgba = cmap(i)\n",
    "    # rgb2hex accepts rgb or rgba\n",
    "    print(matplotlib.colors.rgb2hex(rgba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.groupby([\"HITId\"])[\"complexity\"].mean().value_counts().sort_index()\n",
    "ax = data.plot(kind=\"bar\")\n",
    "\n",
    "new_ticks = np.linspace(0, 9, 10)\n",
    "ax.set_xticks(np.interp(new_ticks, data.index, np.arange(data.size)))\n",
    "ax.set_xticklabels(new_ticks)\n",
    "\n",
    "ax.figure.tight_layout()\n",
    "ax.set_xlabel(\"Complexity\")\n",
    "ax.set_ylabel(\"Amount\")\n",
    "\n",
    "plt.xticks(rotation = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Worker Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"WorkerId\"].value_counts().value_counts().sort_values(), len(df[\"WorkerId\"].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evaluation-YgJUJTN8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
